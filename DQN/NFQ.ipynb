{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Neural Fitted Q Iteration\n",
    "\"\"\"\n",
    "Reinforcement learning with function approximators poses two critical challenges. \n",
    "Not only do we need to fit the function approximator to iteratively observed new data, \n",
    "we also need to make sure that the function approximator does not un-learn or forget \n",
    "in other parts of the state and actions space. \n",
    "This is particularly difficult for (large) neural network function approximators and \n",
    "for a long time these issues have limited reinforcement learning to small function \n",
    "approximator models with few parameters. \n",
    "For temporal-difference learning, a common and important approach to address these problems \n",
    "is storing and re-using transitions and this technique will be in the focus here.\n",
    "In this task, you are asked to re-implement one of the early and seminal papers about \n",
    "using neural networks for temporal-difference learning. If you have not done so already, \n",
    "read the paper\n",
    "\n",
    "Riedmiller, Martin. \n",
    "\"Neural fitted Q iterationâ€“first experiences with a data efficient neural reinforcement learning method.\" \n",
    "In European Conference on Machine Learning, pp. 317-328. Springer, Berlin, Heidelberg, 2005.\n",
    "\n",
    "Solution of Open AI gym environment \"Cartpole-v0\" \n",
    "(https://gym.openai.com/envs/CartPole-v0) using NFQ and Pytorch.:\n",
    "\n",
    "1. Set up a gym environment with (discrete and) finite action space and continuous state space such as \n",
    "CartPole-v0 or MountainCar-v0 and make sure that you can control the apparatus in the gym correctly.\n",
    "\n",
    "2. Implement the NFQ algorithm and make the necessary adaptions. \n",
    "(You might want to take inspiration from the paper and make some adaptions to the reward function.) \n",
    "As the function approximator use a neural network model that you construct in PyTorch and \n",
    "be prepared to try different variants.\n",
    "\n",
    "3. Train NFQ with the gym using the (low-dimensional) state representation provided by the gym. \n",
    "Conduct a thorough investigation with trying out different parameter settings and \n",
    "collecting and recording the necessary information from the learning process needed to determine what works, what does not work, and why. Share and discuss your findings in the discussion board if you have problems or if you find something interesting.\n",
    "\n",
    "4. Optional - This step is optional but very helpful for the next homework assignment. Repeat step 3 but this time use the (high-dimensional) rendered visual representation as the state space. You will have to find out how to access the images for the next task anyway.\n",
    "\n",
    "5. Submit your implementation for this assignment. Take notes and records of your results and bring them to the second session. Be prepared to present and explain your implementation, your findings, and your conclusions. Keep your notes and records for preparing the seminar presentation at the final session where you will be asked to compare to another algorithm.\n",
    "\"\"\"\n",
    "# Imports here\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPISODES = 200  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "GAMMA = 0.8  # Q-learning discount factor\n",
    "LR = 0.001  # NN optimizer learning rate\n",
    "HIDDEN_LAYER = 256  # NN hidden layer size\n",
    "BATCH_SIZE = 64  # Q-learning batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "#ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "#Tensor = FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    This class if for storing transitions. \n",
    "    \n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    says_str : str\n",
    "        a formatted string to print out what the animal says\n",
    "    name : str\n",
    "        the name of the animal\n",
    "    sound : str\n",
    "        the sound that the animal makes\n",
    "    num_legs : int\n",
    "        the number of legs the animal has (default 4)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    says(sound=None)\n",
    "        Prints the animals name and what sound it makes\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int, optional\n",
    "            The upper bound or the capacity of the Memory\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"\n",
    "        Add a transition to the memory and \n",
    "        pop the oldest transition if the capacity limit is reached. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition : transitionObject\n",
    "            The transition to add to the Memory\n",
    "        \"\"\"\n",
    "        self.memory.append(transition)\n",
    "        if 0 < capacity:\n",
    "            if len(self.memory) > self.capacity:\n",
    "                del self.memory[0]\n",
    "        # Capacity = 0 - no upper bound. The memory will keep increasing\n",
    "\n",
    "    def random_draw(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.l1 = nn.Linear(4, HIDDEN_LAYER)\n",
    "        self.l2 = nn.Linear(HIDDEN_LAYER, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n",
    "class Net_CNN_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 6 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        # Second 2D convolutional layer, taking in the 6 input layers,\n",
    "        # outputting 16 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        # 28*28 -> 13*13 -> 5*5...  \n",
    "        # Why? https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, './tmp/cartpole-v0-1', force=True) # Delete video_callalbe=False to render while training\n",
    "\n",
    "model = Network()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "memory = ReplayMemory(10000)\n",
    "# If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. \n",
    "# Parameters of a model after .cuda() will be different objects with those before the call.\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "#optimizer = optim.Rprop(model.parameters(), lr=LR)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to adjust learning rate\n",
    "torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements.\n",
    "\n",
    "Learning rate scheduling should be applied after optimizerâ€™s update; e.g., you should write your code this way:\n",
    "\n",
    ">>> scheduler = ...\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(e, environment):\n",
    "    state = environment.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        environment.render()\n",
    "        action = select_action(FloatTensor([state]))\n",
    "        next_state, reward, done, _ = environment.step(action[0, 0])\n",
    "\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        memory.push((FloatTensor([state]),\n",
    "                     action,  # action is already a tensor\n",
    "                     FloatTensor([next_state]),\n",
    "                     FloatTensor([reward])))\n",
    "\n",
    "        learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(e, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "            episode_durations.append(steps)\n",
    "            plot_durations()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "\n",
    "    batch_state = Variable(torch.cat(batch_state))\n",
    "    batch_action = Variable(torch.cat(batch_action))\n",
    "    batch_reward = Variable(torch.cat(batch_reward))\n",
    "    batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\n",
    "    # current Q values are estimated by NN for all actions\n",
    "    current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    # expected Q values are estimated from actions which gives maximum Q value\n",
    "    max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
    "    expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(current_q_values, expected_q_values)\n",
    "\n",
    "    # backpropagation of loss to NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "tensor(0, device='cuda:0') (<class 'torch.Tensor'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a49633a253e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Complete'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-9a8e4fc1b602>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m(e, environment)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# negative reward when attempt ends\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%r (%s) invalid\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: tensor(0, device='cuda:0') (<class 'torch.Tensor'>) invalid"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    print('Episode: ', e)\n",
    "    run_episode(e, env)\n",
    "\n",
    "print('Complete')\n",
    "env.render(close=True)\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
